// #define vidcap
#ifdef vidcap
    VideoCapture vidCap;
    vidCap.open(0);
    cv::Mat output;
    int counter = 0;
    while(true){
        vidCap >> output;
        auto rectX = output.cols / 2 - (92);
        auto rectY = output.rows / 2 - (112);
        auto rect = Rect(rectX, rectY, 92 * 2, 112 * 2);
        auto display = output.clone();
        rectangle(display, rect, CV_RGB(0, 255,0), 1);
        cv::imshow("Gray", display);

        if (cv::waitKey(5) == 120) { // z
            cv::cvtColor(output, output, cv::COLOR_BGR2GRAY); // conv color to gray
            Mat face = output(rect);
            cv::resize(face, face, cv::Size(), 0.5, 0.5);
            string name = to_string(counter) + ".png";
            cv::imwrite(name, face);
            counter ++;
            cout << "Image saved" << endl;
        };

        if (cv::waitKey(5) == 122) { // x
            cout << "Image Discarded" << endl;
        }

        if (cv::waitKey(5) == 113) break; // stop capturing by pressing ESC
    }

    vidCap.release();
    cv::destroyAllWindows();


    // Get the height from the first image. We'll need this
    // later in code to reshape the images to their originalqqq
    // size AND we need to reshape incoming faces to this size:
    int im_width = images[0].cols;
    int im_height = images[0].rows;
    
    // Create a FaceRecognizer and train it on the given images:
    Ptr<cv::face::FaceRecognizer> model = cv::face::FisherFaceRecognizer::create();
    model->train(images, labels);
    // That's it for learning the Face Recognition model. You now
    // need to create the classifier for the task of Face Detection.
    // We are going to use the haar cascade you have specified in the
    // command line arguments:
    //
    CascadeClassifier haar_cascade;
    cout << "loading haar cascade at location: " << fn_haar << endl;
    auto result = haar_cascade.load(fn_haar);
    cout << "Haar cascade loaded with result: " << result << endl;
    // Get a handle to the Video device:
    VideoCapture cap(deviceId);
    // Check if we can use this device at all:
    if(!cap.isOpened()) {
        cerr << "Capture Device ID " << deviceId << "cannot be opened." << endl;
        return -1;
    }
    // Holds the current frame from the Video device:
    Mat frame;
    for(;;) {
        cap >> frame;
        // Clone the current frame:
        Mat original = frame.clone();
        // Convert the current frame to grayscale:
        Mat gray;
        cvtColor(original, gray, COLOR_BGR2GRAY);
        // Find the faces in the frame:
        vector< Rect_<int> > faces;
        vector<int> rejectLevels;
        vector<double> confidence;
        const Size min_size = Size(48, 48);
        const Size max_size = Size();
        haar_cascade.detectMultiScale(gray, faces, rejectLevels, confidence, 1.1, 3, 0, min_size, max_size, true);
        // At this point you have the position of the faces in
        // faces. Now we'll get the faces, make a prediction and
        // annotate it in the video. Cool or what?
        for(int i = 0; i < faces.size(); i++) {
            // Process face by face:
            Rect face_i = faces[i];
            // Crop the face from the image. So simple with OpenCV C++:
            Mat face = gray(face_i);
            // Resizing the face is necessary for Eigenfaces and Fisherfaces. You can easily
            // verify this, by reading through the face recognition tutorial coming with OpenCV.
            // Resizing IS NOT NEEDED for Local Binary Patterns Histograms, so preparing the
            // input data really depends on the algorithm used.
            //
            // I strongly encourage you to play around with the algorithms. See which work best
            // in your scenario, LBPH should always be a contender for robust face recognition.
            //
            // Since I am showing the Fisherfaces algorithm here, I also show how to resize the
            // face you have just found:
            Mat face_resized;
            cv::resize(face, face_resized, Size(im_width, im_height), 1.0, 1.0, INTER_CUBIC);
            // Now perform the prediction, see how easy that is:
            int prediction = model->predict(face_resized);
            // And finally write all we've found out to the original image!
            // First of all draw a green rectangle around the detected face:
            rectangle(original, face_i, CV_RGB(0, 255,0), 1);
            // Create the text we will annotate the box with:
            cerr << "Detection " << faces[i] << " with weight " << confidence[i] << endl;
            string box_text = format("Prediction = %d", prediction);
            // Calculate the position for annotated text (make sure we don't
            // put illegal values in there):
            int pos_x = std::max(face_i.tl().x - 10, 0);
            int pos_y = std::max(face_i.tl().y - 10, 0);
            // And now put it into the image:
            putText(original, box_text, Point(pos_x, pos_y), FONT_HERSHEY_PLAIN, 1.0, CV_RGB(0,255,0), 2.0);
        }
        // Show the result:
        imshow("face_recognizer", original);
        // And display it:
        char key = (char) waitKey(20);
        // Exit this loop on escape:
        if(key == 27)
            break;
    }
#endif
